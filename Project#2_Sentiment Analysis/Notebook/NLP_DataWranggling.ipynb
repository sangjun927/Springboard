{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1072b704",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79ee26a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "# gradient boosting for classification in scikit-learn\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import concurrent.futures\n",
    "import time\n",
    "import threading\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost\n",
    "from sklearn import svm\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ef957d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load excel\n",
    "data = pd.read_csv(\"/Users/hansangjun/Desktop/Projects/Capstone_Project_2/Data/IMDB_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6d1fcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09922eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7d8313",
   "metadata": {},
   "source": [
    "Categorical variable has been equally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f16a536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace 1 and 0\n",
    "data.sentiment = data.sentiment.replace({'positive': 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0de66098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    25000\n",
       "0    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cafde87",
   "metadata": {},
   "source": [
    "## 2. Subset Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c9f834",
   "metadata": {},
   "source": [
    "I am going to subset the dataset to efficiently run the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "806dfe73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5021\n",
       "0    4979\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data.sample(n=10000, random_state=123)\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199baecb",
   "metadata": {},
   "source": [
    "## 3. Before Cleaning Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa74ab9",
   "metadata": {},
   "source": [
    "### 3-1. CountVectorizor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddc63baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'].values, df['sentiment'].values, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e728070b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7500x46228 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1016334 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test  = vectorizer.transform(X_test)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de643964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 46228)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c428dc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    RandomForestClassifier(),\n",
    "    XGBClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    LogisticRegression(),\n",
    "    naive_bayes.MultinomialNB()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e91287ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name: KNeighborsClassifier(n_neighbors=3)\n",
      "Accuracy: 60.7600%\n",
      "Log Loss: 4.177093981246377\n",
      "--- 3.2576897144317627 seconds ---\n",
      "========================================\n",
      "model name: RandomForestClassifier()\n",
      "Accuracy: 83.8400%\n",
      "Log Loss: 0.5038459155699745\n",
      "--- 7.72878623008728 seconds ---\n",
      "========================================\n",
      "model name: XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
      "              grow_policy='depthwise', importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0, ...)\n",
      "Accuracy: 84.2400%\n",
      "Log Loss: 0.36178426572654715\n",
      "--- 6.43055272102356 seconds ---\n",
      "========================================\n",
      "model name: AdaBoostClassifier()\n",
      "Accuracy: 79.6000%\n",
      "Log Loss: 0.6765695252104681\n",
      "--- 2.7040979862213135 seconds ---\n",
      "========================================\n",
      "model name: GradientBoostingClassifier()\n",
      "Accuracy: 80.1600%\n",
      "Log Loss: 0.45789180252484796\n",
      "--- 10.015100955963135 seconds ---\n",
      "========================================\n",
      "model name: LogisticRegression()\n",
      "Accuracy: 87.3600%\n",
      "Log Loss: 0.3997934331752452\n",
      "--- 0.919187068939209 seconds ---\n",
      "========================================\n",
      "model name: MultinomialNB()\n",
      "Accuracy: 84.0000%\n",
      "Log Loss: 1.2204450408573044\n",
      "--- 0.016714096069335938 seconds ---\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hansangjun/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "for clf in classifiers:\n",
    "    start_time = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"model name: {}\".format(clf))\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "\n",
    "    train_predictions = clf.predict_proba(X_test)\n",
    "    ll = log_loss(y_test, train_predictions)\n",
    "    print(\"Log Loss: {}\".format(ll))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff0b5bc",
   "metadata": {},
   "source": [
    "### 3-2 TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b78dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['review'].values, df['sentiment'].values, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09fd3302",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer() \n",
    "tfidf_train_vectors = tfidf_vectorizer.fit_transform(X_train)\n",
    "tfidf_test_vectors = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a166a658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name: KNeighborsClassifier(n_neighbors=3)\n",
      "Accuracy: 68.0000%\n",
      "Log Loss: 3.1417675313828215\n",
      "--- 3.2920122146606445 seconds ---\n",
      "========================================\n",
      "model name: RandomForestClassifier()\n",
      "Accuracy: 83.0400%\n",
      "Log Loss: 0.5166687142492233\n",
      "--- 7.693850994110107 seconds ---\n",
      "========================================\n",
      "model name: XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
      "              grow_policy='depthwise', importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0, ...)\n",
      "Accuracy: 83.0000%\n",
      "Log Loss: 0.3683363966465229\n",
      "--- 17.34945011138916 seconds ---\n",
      "========================================\n",
      "model name: AdaBoostClassifier()\n",
      "Accuracy: 79.1200%\n",
      "Log Loss: 0.6750909654237487\n",
      "--- 5.856276988983154 seconds ---\n",
      "========================================\n",
      "model name: GradientBoostingClassifier()\n",
      "Accuracy: 79.7600%\n",
      "Log Loss: 0.45689846798303263\n",
      "--- 26.327625036239624 seconds ---\n",
      "========================================\n",
      "model name: LogisticRegression()\n",
      "Accuracy: 88.5600%\n",
      "Log Loss: 0.3778224779320997\n",
      "--- 0.4116489887237549 seconds ---\n",
      "========================================\n",
      "model name: MultinomialNB()\n",
      "Accuracy: 86.5200%\n",
      "Log Loss: 0.44554468654226476\n",
      "--- 0.015521049499511719 seconds ---\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "for clf in classifiers:\n",
    "    start_time = time.time()\n",
    "    clf.fit(tfidf_train_vectors, y_train)\n",
    "    train_predictions = clf.predict(tfidf_test_vectors)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"model name: {}\".format(clf))\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "\n",
    "    train_predictions = clf.predict_proba(tfidf_test_vectors)\n",
    "    ll = log_loss(y_test, train_predictions)\n",
    "    print(\"Log Loss: {}\".format(ll))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efe0ae9",
   "metadata": {},
   "source": [
    "## 4. After Cleaning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9acccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "\n",
    "    text = re.sub(r\"http\\S+\", \"\",text) # Removing URLs \n",
    "    #text = re.sub(r\"http\", \"\",text)\n",
    "    \n",
    "    html=re.compile(r'<.*?>') \n",
    "    \n",
    "    text = html.sub(r'',text) # Removing html tags\n",
    "    \n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p,'') #Removing punctuations\n",
    "        \n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    \n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    \n",
    "    text = \" \".join(text) #removing stopwords\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "589b6031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 11.170544147491455 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11872</th>\n",
       "      <td>movie beyond awful, pimple movie industry know...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40828</th>\n",
       "      <td>writing john carpenter halloween nearing th an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36400</th>\n",
       "      <td>must admit slight disappointment film read lot...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5166</th>\n",
       "      <td>oh dear bbc knocked pedestal absorbing period ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30273</th>\n",
       "      <td>totally average film semi alright action seque...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "11872  movie beyond awful, pimple movie industry know...          0\n",
       "40828  writing john carpenter halloween nearing th an...          1\n",
       "36400  must admit slight disappointment film read lot...          1\n",
       "5166   oh dear bbc knocked pedestal absorbing period ...          0\n",
       "30273  totally average film semi alright action seque...          0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "df['review'] = df['review'].apply(lambda x: clean_text(x))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfee4f0",
   "metadata": {},
   "source": [
    "### 4-1. CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0506036",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['review'].values, df['sentiment'].values, test_size=0.25, random_state=1000)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test  = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7fc42388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name: KNeighborsClassifier(n_neighbors=3)\n",
      "Accuracy: 57.8800%\n",
      "Log Loss: 5.465913433661351\n",
      "--- 2.382977247238159 seconds ---\n",
      "========================================\n",
      "model name: RandomForestClassifier()\n",
      "Accuracy: 84.8400%\n",
      "Log Loss: 0.4819815269797356\n",
      "--- 7.414118051528931 seconds ---\n",
      "========================================\n",
      "model name: XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
      "              grow_policy='depthwise', importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0, ...)\n",
      "Accuracy: 82.2400%\n",
      "Log Loss: 0.3714679159926425\n",
      "--- 4.852926969528198 seconds ---\n",
      "========================================\n",
      "model name: AdaBoostClassifier()\n",
      "Accuracy: 78.8400%\n",
      "Log Loss: 0.6766049179988348\n",
      "--- 1.9580910205841064 seconds ---\n",
      "========================================\n",
      "model name: GradientBoostingClassifier()\n",
      "Accuracy: 79.7600%\n",
      "Log Loss: 0.4607563557662259\n",
      "--- 7.840910911560059 seconds ---\n",
      "========================================\n",
      "model name: LogisticRegression()\n",
      "Accuracy: 86.7600%\n",
      "Log Loss: 0.3974850716022048\n",
      "--- 0.7656450271606445 seconds ---\n",
      "========================================\n",
      "model name: MultinomialNB()\n",
      "Accuracy: 85.3600%\n",
      "Log Loss: 1.0200985230893476\n",
      "--- 0.01710796356201172 seconds ---\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hansangjun/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "for clf in classifiers:\n",
    "    start_time = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"model name: {}\".format(clf))\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "\n",
    "    train_predictions = clf.predict_proba(X_test)\n",
    "    ll = log_loss(y_test, train_predictions)\n",
    "    print(\"Log Loss: {}\".format(ll))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a703144",
   "metadata": {},
   "source": [
    "### 4-2 TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8633324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['review'].values, df['sentiment'].values, test_size=0.25, random_state=1000)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer() \n",
    "tfidf_train_vectors = tfidf_vectorizer.fit_transform(X_train)\n",
    "tfidf_test_vectors = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b621e15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name: KNeighborsClassifier(n_neighbors=3)\n",
      "Accuracy: 70.9200%\n",
      "Log Loss: 3.1383864163576263\n",
      "--- 2.303748846054077 seconds ---\n",
      "========================================\n",
      "model name: RandomForestClassifier()\n",
      "Accuracy: 84.5600%\n",
      "Log Loss: 0.4889608416785859\n",
      "--- 6.98526406288147 seconds ---\n",
      "========================================\n",
      "model name: XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
      "              grow_policy='depthwise', importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0, ...)\n",
      "Accuracy: 82.6400%\n",
      "Log Loss: 0.37218234992326504\n",
      "--- 11.178217887878418 seconds ---\n",
      "========================================\n",
      "model name: AdaBoostClassifier()\n",
      "Accuracy: 78.2400%\n",
      "Log Loss: 0.6756066928683364\n",
      "--- 4.182494878768921 seconds ---\n",
      "========================================\n",
      "model name: GradientBoostingClassifier()\n",
      "Accuracy: 79.2800%\n",
      "Log Loss: 0.4621487854590324\n",
      "--- 18.375642776489258 seconds ---\n",
      "========================================\n",
      "model name: LogisticRegression()\n",
      "Accuracy: 87.5200%\n",
      "Log Loss: 0.38128143079532956\n",
      "--- 0.39110875129699707 seconds ---\n",
      "========================================\n",
      "model name: MultinomialNB()\n",
      "Accuracy: 87.0800%\n",
      "Log Loss: 0.436983315965707\n",
      "--- 0.015234947204589844 seconds ---\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "for clf in classifiers:\n",
    "    start_time = time.time()\n",
    "    clf.fit(tfidf_train_vectors, y_train)\n",
    "    train_predictions = clf.predict(tfidf_test_vectors)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"model name: {}\".format(clf))\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "\n",
    "    train_predictions = clf.predict_proba(tfidf_test_vectors)\n",
    "    ll = log_loss(y_test, train_predictions)\n",
    "    print(\"Log Loss: {}\".format(ll))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfce8eba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
