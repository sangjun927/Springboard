{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/whsos927/notebook96856d2406?scriptVersionId=116056162\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"**Imports**","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-10T23:13:49.233067Z","iopub.execute_input":"2023-01-10T23:13:49.233685Z","iopub.status.idle":"2023-01-10T23:13:49.265296Z","shell.execute_reply.started":"2023-01-10T23:13:49.233565Z","shell.execute_reply":"2023-01-10T23:13:49.264425Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# load excel\ndata = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:13:49.362394Z","iopub.execute_input":"2023-01-10T23:13:49.362727Z","iopub.status.idle":"2023-01-10T23:13:51.307974Z","shell.execute_reply.started":"2023-01-10T23:13:49.362695Z","shell.execute_reply":"2023-01-10T23:13:51.30703Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install keras","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:13:51.309652Z","iopub.execute_input":"2023-01-10T23:13:51.30991Z","iopub.status.idle":"2023-01-10T23:14:02.415634Z","shell.execute_reply.started":"2023-01-10T23:13:51.309878Z","shell.execute_reply":"2023-01-10T23:14:02.414599Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting keras\n  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n\u001b[K     |████████████████████████████████| 1.7 MB 2.0 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: keras\nSuccessfully installed keras-2.11.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loading Dependencies\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport transformers\n\nfrom tokenizers import BertWordPieceTokenizer\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:14:02.417108Z","iopub.execute_input":"2023-01-10T23:14:02.417411Z","iopub.status.idle":"2023-01-10T23:14:12.109338Z","shell.execute_reply.started":"2023-01-10T23:14:02.417374Z","shell.execute_reply":"2023-01-10T23:14:12.108078Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2023-01-10 23:14:03.134505: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2023-01-10 23:14:03.134721: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:14:12.112502Z","iopub.execute_input":"2023-01-10T23:14:12.113091Z","iopub.status.idle":"2023-01-10T23:14:16.798778Z","shell.execute_reply.started":"2023-01-10T23:14:12.113034Z","shell.execute_reply":"2023-01-10T23:14:16.797881Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Running on TPU  grpc://10.0.0.2:8470\n","output_type":"stream"},{"name":"stderr","text":"2023-01-10 23:14:12.122200: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2023-01-10 23:14:12.125319: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2023-01-10 23:14:12.125364: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n2023-01-10 23:14:12.125395: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (4e3274f68159): /proc/driver/nvidia/version does not exist\n2023-01-10 23:14:12.129325: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-01-10 23:14:12.130850: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2023-01-10 23:14:12.162521: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2023-01-10 23:14:12.162600: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30020}\n2023-01-10 23:14:12.180576: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2023-01-10 23:14:12.180633: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30020}\n2023-01-10 23:14:12.182189: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:30020\n","output_type":"stream"},{"name":"stdout","text":"REPLICAS:  8\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Arrange and subset dataset**","metadata":{}},{"cell_type":"code","source":"# replace 1 and 0\ndata.sentiment = data.sentiment.replace({'positive': 1, 'negative': 0})","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:14:16.800009Z","iopub.execute_input":"2023-01-10T23:14:16.800272Z","iopub.status.idle":"2023-01-10T23:14:16.864008Z","shell.execute_reply.started":"2023-01-10T23:14:16.800242Z","shell.execute_reply":"2023-01-10T23:14:16.863001Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Creating training and valid dataset by ratio of 1:9\n# valid dataset\nvalid = data.sample(frac = 0.1)\n \n# Creating dataframe with\n# training dataset\ntrain1 = data.drop(valid.index)\n \nprint(\"\\n10% of the given DataFrame:\")\nprint(valid.shape)\n \nprint(\"\\nrest 90% of the given DataFrame:\")\nprint(train1.shape)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:14:16.865102Z","iopub.execute_input":"2023-01-10T23:14:16.865336Z","iopub.status.idle":"2023-01-10T23:14:16.882111Z","shell.execute_reply.started":"2023-01-10T23:14:16.86531Z","shell.execute_reply":"2023-01-10T23:14:16.881244Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\n10% of the given DataFrame:\n(5000, 2)\n\nrest 90% of the given DataFrame:\n(45000, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"valid.sentiment.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:14:16.883555Z","iopub.execute_input":"2023-01-10T23:14:16.883814Z","iopub.status.idle":"2023-01-10T23:14:16.896446Z","shell.execute_reply.started":"2023-01-10T23:14:16.883786Z","shell.execute_reply":"2023-01-10T23:14:16.895428Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"1    2507\n0    2493\nName: sentiment, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train1.sentiment.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:14:16.897881Z","iopub.execute_input":"2023-01-10T23:14:16.898105Z","iopub.status.idle":"2023-01-10T23:14:16.908442Z","shell.execute_reply.started":"2023-01-10T23:14:16.89808Z","shell.execute_reply":"2023-01-10T23:14:16.907576Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"0    22507\n1    22493\nName: sentiment, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train1 = train1.reset_index(drop=True)\nvalid = valid.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:14:16.909767Z","iopub.execute_input":"2023-01-10T23:14:16.910111Z","iopub.status.idle":"2023-01-10T23:14:16.921798Z","shell.execute_reply.started":"2023-01-10T23:14:16.910068Z","shell.execute_reply":"2023-01-10T23:14:16.921109Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"valid.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:14:16.92511Z","iopub.execute_input":"2023-01-10T23:14:16.925373Z","iopub.status.idle":"2023-01-10T23:14:16.942016Z","shell.execute_reply.started":"2023-01-10T23:14:16.925343Z","shell.execute_reply":"2023-01-10T23:14:16.941026Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                              review  sentiment\n0  Note, I only saw approximately the last half o...          0\n1  One hilarious thing I'll say off the top, is I...          1\n2  Absolutely one of the 10 best music films Ever...          1\n3  I bought the DVD version of this movie on the ...          0\n4  Can we say retarded? This girl has no talent w...          0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Note, I only saw approximately the last half o...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>One hilarious thing I'll say off the top, is I...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Absolutely one of the 10 best music films Ever...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I bought the DVD version of this movie on the ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Can we say retarded? This girl has no talent w...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Encoding**","metadata":{}},{"cell_type":"code","source":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    \"\"\"\n    Encoder for encoding the text into sequence of integers for BERT Input\n    \"\"\"\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:14:16.943975Z","iopub.execute_input":"2023-01-10T23:14:16.944212Z","iopub.status.idle":"2023-01-10T23:14:16.952278Z","shell.execute_reply.started":"2023-01-10T23:14:16.944178Z","shell.execute_reply":"2023-01-10T23:14:16.951215Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#IMP DATA FOR CONFIG\n\nAUTO = tf.data.experimental.AUTOTUNE\n\n\n# Configuration\nEPOCHS = 3\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 192","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:14:16.953544Z","iopub.execute_input":"2023-01-10T23:14:16.953793Z","iopub.status.idle":"2023-01-10T23:14:16.962643Z","shell.execute_reply.started":"2023-01-10T23:14:16.953764Z","shell.execute_reply":"2023-01-10T23:14:16.96161Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"**Tokenization**","metadata":{}},{"cell_type":"code","source":"# First load the real tokenizer\ntokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\nfast_tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:14:16.963904Z","iopub.execute_input":"2023-01-10T23:14:16.964143Z","iopub.status.idle":"2023-01-10T23:14:19.449303Z","shell.execute_reply.started":"2023-01-10T23:14:16.964116Z","shell.execute_reply":"2023-01-10T23:14:19.448286Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ce3be3292694d9b9bef45ddc5dec45f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"003c81430cf443b08361e1569783fb0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea802b36c5d249b18143cae2a4d675f4"}},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"Tokenizer(vocabulary_size=119547, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=False, wordpieces_prefix=##)"},"metadata":{}}]},{"cell_type":"code","source":"x_train = fast_encode(train1.review.astype(str), fast_tokenizer, maxlen=MAX_LEN)\nx_valid = fast_encode(valid.review.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n#x_test = fast_encode(test1.review.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n\ny_train = train1.sentiment.values\ny_valid = valid.sentiment.values","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:14:19.450974Z","iopub.execute_input":"2023-01-10T23:14:19.451294Z","iopub.status.idle":"2023-01-10T23:14:41.431855Z","shell.execute_reply.started":"2023-01-10T23:14:19.451254Z","shell.execute_reply":"2023-01-10T23:14:41.430927Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"100%|██████████| 176/176 [00:19<00:00,  9.25it/s]\n100%|██████████| 20/20 [00:02<00:00,  9.89it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .shuffle(3000)\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\n# test_dataset = (\n#     tf.data.Dataset\n#     .from_tensor_slices(x_test)\n#     .batch(BATCH_SIZE)\n# )","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:14:41.43507Z","iopub.execute_input":"2023-01-10T23:14:41.435543Z","iopub.status.idle":"2023-01-10T23:14:41.809204Z","shell.execute_reply.started":"2023-01-10T23:14:41.435476Z","shell.execute_reply":"2023-01-10T23:14:41.80805Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"**Metrics with Accuracy**","metadata":{}},{"cell_type":"code","source":"def build_model(transformer, max_len=512):\n    \"\"\"\n    function for training the BERT model\n    \"\"\"\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:14:41.810628Z","iopub.execute_input":"2023-01-10T23:14:41.811259Z","iopub.status.idle":"2023-01-10T23:14:41.818087Z","shell.execute_reply.started":"2023-01-10T23:14:41.811217Z","shell.execute_reply":"2023-01-10T23:14:41.817438Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"%%time\nwith strategy.scope():\n    transformer_layer = (\n        transformers.TFDistilBertModel\n        .from_pretrained('distilbert-base-multilingual-cased')\n    )\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:14:41.819814Z","iopub.execute_input":"2023-01-10T23:14:41.820423Z","iopub.status.idle":"2023-01-10T23:15:50.203234Z","shell.execute_reply.started":"2023-01-10T23:14:41.820379Z","shell.execute_reply":"2023-01-10T23:15:50.20217Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/466 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e32dab5c4074fa581fae5823e6ecf97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/911M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd0b2ce74e444a3ca9a5325599024331"}},"metadata":{}},{"name":"stderr","text":"2023-01-10 23:15:13.987655: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\nSome layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_transform', 'vocab_projector', 'vocab_layer_norm']\n- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_word_ids (InputLayer)  [(None, 192)]             0         \n_________________________________________________________________\ntf_distil_bert_model (TFDist TFBaseModelOutput(last_hi 134734080 \n_________________________________________________________________\ntf.__operators__.getitem (Sl (None, 768)               0         \n_________________________________________________________________\ndense (Dense)                (None, 1)                 769       \n=================================================================\nTotal params: 134,734,849\nTrainable params: 134,734,849\nNon-trainable params: 0\n_________________________________________________________________\nCPU times: user 32.9 s, sys: 25.4 s, total: 58.3 s\nWall time: 1min 8s\n","output_type":"stream"}]},{"cell_type":"code","source":"n_steps = x_train.shape[0] // BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=6\n    #batch_size = BATCH_SIZE\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:15:50.204993Z","iopub.execute_input":"2023-01-10T23:15:50.205323Z","iopub.status.idle":"2023-01-10T23:19:27.802522Z","shell.execute_reply.started":"2023-01-10T23:15:50.205282Z","shell.execute_reply":"2023-01-10T23:19:27.801436Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Epoch 1/6\n351/351 [==============================] - 78s 102ms/step - loss: 0.5372 - accuracy: 0.7039 - val_loss: 0.3288 - val_accuracy: 0.8566\nEpoch 2/6\n351/351 [==============================] - 28s 79ms/step - loss: 0.3190 - accuracy: 0.8601 - val_loss: 0.3037 - val_accuracy: 0.8740\nEpoch 3/6\n351/351 [==============================] - 28s 79ms/step - loss: 0.2622 - accuracy: 0.8880 - val_loss: 0.3053 - val_accuracy: 0.8794\nEpoch 4/6\n351/351 [==============================] - 28s 79ms/step - loss: 0.2227 - accuracy: 0.9059 - val_loss: 0.3149 - val_accuracy: 0.8800\nEpoch 5/6\n351/351 [==============================] - 28s 79ms/step - loss: 0.1841 - accuracy: 0.9282 - val_loss: 0.3320 - val_accuracy: 0.8732\nEpoch 6/6\n351/351 [==============================] - 28s 80ms/step - loss: 0.1486 - accuracy: 0.9404 - val_loss: 0.3384 - val_accuracy: 0.8804\n","output_type":"stream"}]},{"cell_type":"code","source":"n_steps = x_valid.shape[0] // BATCH_SIZE\ntrain_history_2 = model.fit(\n    valid_dataset.repeat(),\n    steps_per_epoch=n_steps,\n    epochs=EPOCHS*2\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:19:27.804083Z","iopub.execute_input":"2023-01-10T23:19:27.804364Z","iopub.status.idle":"2023-01-10T23:20:10.963008Z","shell.execute_reply.started":"2023-01-10T23:19:27.80433Z","shell.execute_reply":"2023-01-10T23:20:10.961977Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Epoch 1/6\n39/39 [==============================] - 3s 74ms/step - loss: 0.3042 - accuracy: 0.8732\nEpoch 2/6\n39/39 [==============================] - 28s 74ms/step - loss: 0.2386 - accuracy: 0.8990\nEpoch 3/6\n39/39 [==============================] - 3s 74ms/step - loss: 0.1912 - accuracy: 0.9232\nEpoch 4/6\n39/39 [==============================] - 3s 73ms/step - loss: 0.1437 - accuracy: 0.9433\nEpoch 5/6\n39/39 [==============================] - 3s 74ms/step - loss: 0.1140 - accuracy: 0.9565\nEpoch 6/6\n39/39 [==============================] - 3s 74ms/step - loss: 0.0851 - accuracy: 0.9692\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Metrics with recall**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import backend as K\n#tensorflow.keras.\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:20:10.964759Z","iopub.execute_input":"2023-01-10T23:20:10.965023Z","iopub.status.idle":"2023-01-10T23:20:10.970877Z","shell.execute_reply.started":"2023-01-10T23:20:10.964993Z","shell.execute_reply":"2023-01-10T23:20:10.970199Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def build_model(transformer, max_len=512):\n    \"\"\"\n    function for training the BERT model\n    \"\"\"\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=[recall_m])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:20:10.97229Z","iopub.execute_input":"2023-01-10T23:20:10.972566Z","iopub.status.idle":"2023-01-10T23:20:10.986864Z","shell.execute_reply.started":"2023-01-10T23:20:10.972533Z","shell.execute_reply":"2023-01-10T23:20:10.985811Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"%%time\nwith strategy.scope():\n    transformer_layer = (\n        transformers.TFDistilBertModel\n        .from_pretrained('distilbert-base-multilingual-cased')\n    )\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:20:10.988547Z","iopub.execute_input":"2023-01-10T23:20:10.988891Z","iopub.status.idle":"2023-01-10T23:20:25.983601Z","shell.execute_reply.started":"2023-01-10T23:20:10.98885Z","shell.execute_reply":"2023-01-10T23:20:25.982589Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_transform', 'vocab_projector', 'vocab_layer_norm']\n- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_word_ids (InputLayer)  [(None, 192)]             0         \n_________________________________________________________________\ntf_distil_bert_model_1 (TFDi TFBaseModelOutput(last_hi 134734080 \n_________________________________________________________________\ntf.__operators__.getitem_1 ( (None, 768)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 769       \n=================================================================\nTotal params: 134,734,849\nTrainable params: 134,734,849\nNon-trainable params: 0\n_________________________________________________________________\nCPU times: user 7.7 s, sys: 7.87 s, total: 15.6 s\nWall time: 15 s\n","output_type":"stream"}]},{"cell_type":"code","source":"n_steps = x_train.shape[0] // BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=6\n    #batch_size = BATCH_SIZE\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:20:25.985127Z","iopub.execute_input":"2023-01-10T23:20:25.985662Z","iopub.status.idle":"2023-01-10T23:24:03.089478Z","shell.execute_reply.started":"2023-01-10T23:20:25.985617Z","shell.execute_reply":"2023-01-10T23:24:03.08859Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Epoch 1/6\n351/351 [==============================] - 78s 102ms/step - loss: 0.5756 - recall_m: 0.6697 - val_loss: 0.3269 - val_recall_m: 0.8386\nEpoch 2/6\n351/351 [==============================] - 28s 79ms/step - loss: 0.3209 - recall_m: 0.8596 - val_loss: 0.3072 - val_recall_m: 0.8778\nEpoch 3/6\n351/351 [==============================] - 28s 79ms/step - loss: 0.2631 - recall_m: 0.8929 - val_loss: 0.3045 - val_recall_m: 0.8848\nEpoch 4/6\n351/351 [==============================] - 28s 79ms/step - loss: 0.2205 - recall_m: 0.9136 - val_loss: 0.3205 - val_recall_m: 0.8863\nEpoch 5/6\n351/351 [==============================] - 28s 79ms/step - loss: 0.1851 - recall_m: 0.9278 - val_loss: 0.3466 - val_recall_m: 0.8386\nEpoch 6/6\n351/351 [==============================] - 28s 79ms/step - loss: 0.1426 - recall_m: 0.9452 - val_loss: 0.3541 - val_recall_m: 0.8733\n","output_type":"stream"}]},{"cell_type":"code","source":"n_steps = x_valid.shape[0] // BATCH_SIZE\ntrain_history_2 = model.fit(\n    valid_dataset.repeat(),\n    steps_per_epoch=n_steps,\n    epochs=EPOCHS*2\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:24:03.093239Z","iopub.execute_input":"2023-01-10T23:24:03.093545Z","iopub.status.idle":"2023-01-10T23:24:47.159601Z","shell.execute_reply.started":"2023-01-10T23:24:03.093503Z","shell.execute_reply":"2023-01-10T23:24:47.158559Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Epoch 1/6\n39/39 [==============================] - 3s 74ms/step - loss: 0.3103 - recall_m: 0.8782\nEpoch 2/6\n39/39 [==============================] - 29s 75ms/step - loss: 0.2370 - recall_m: 0.8906\nEpoch 3/6\n39/39 [==============================] - 3s 73ms/step - loss: 0.1939 - recall_m: 0.9034\nEpoch 4/6\n39/39 [==============================] - 3s 73ms/step - loss: 0.1604 - recall_m: 0.9166\nEpoch 5/6\n39/39 [==============================] - 3s 73ms/step - loss: 0.1416 - recall_m: 0.9271\nEpoch 6/6\n39/39 [==============================] - 3s 74ms/step - loss: 0.1591 - recall_m: 0.9198\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Metrics with F1 Score**","metadata":{}},{"cell_type":"code","source":"def f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:24:47.16119Z","iopub.execute_input":"2023-01-10T23:24:47.161421Z","iopub.status.idle":"2023-01-10T23:24:47.167273Z","shell.execute_reply.started":"2023-01-10T23:24:47.161394Z","shell.execute_reply":"2023-01-10T23:24:47.16658Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def build_model(transformer, max_len=512):\n    \"\"\"\n    function for training the BERT model\n    \"\"\"\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=[f1_m])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:24:47.16833Z","iopub.execute_input":"2023-01-10T23:24:47.168943Z","iopub.status.idle":"2023-01-10T23:24:47.183272Z","shell.execute_reply.started":"2023-01-10T23:24:47.168905Z","shell.execute_reply":"2023-01-10T23:24:47.182603Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"%%time\nwith strategy.scope():\n    transformer_layer = (\n        transformers.TFDistilBertModel\n        .from_pretrained('distilbert-base-multilingual-cased')\n    )\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:24:47.18464Z","iopub.execute_input":"2023-01-10T23:24:47.185472Z","iopub.status.idle":"2023-01-10T23:25:00.644437Z","shell.execute_reply.started":"2023-01-10T23:24:47.185428Z","shell.execute_reply":"2023-01-10T23:25:00.643439Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_transform', 'vocab_projector', 'vocab_layer_norm']\n- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_word_ids (InputLayer)  [(None, 192)]             0         \n_________________________________________________________________\ntf_distil_bert_model_2 (TFDi TFBaseModelOutput(last_hi 134734080 \n_________________________________________________________________\ntf.__operators__.getitem_2 ( (None, 768)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 769       \n=================================================================\nTotal params: 134,734,849\nTrainable params: 134,734,849\nNon-trainable params: 0\n_________________________________________________________________\nCPU times: user 7.44 s, sys: 7.95 s, total: 15.4 s\nWall time: 13.4 s\n","output_type":"stream"}]},{"cell_type":"code","source":"n_steps = x_train.shape[0] // BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=6\n    #batch_size = BATCH_SIZE\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:25:00.647888Z","iopub.execute_input":"2023-01-10T23:25:00.648183Z","iopub.status.idle":"2023-01-10T23:28:41.59508Z","shell.execute_reply.started":"2023-01-10T23:25:00.64815Z","shell.execute_reply":"2023-01-10T23:28:41.594057Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Epoch 1/6\n351/351 [==============================] - 81s 103ms/step - loss: 0.5513 - f1_m: 0.6641 - val_loss: 0.3292 - val_f1_m: 0.8276\nEpoch 2/6\n351/351 [==============================] - 28s 79ms/step - loss: 0.3166 - f1_m: 0.8523 - val_loss: 0.3096 - val_f1_m: 0.8469\nEpoch 3/6\n351/351 [==============================] - 28s 79ms/step - loss: 0.2668 - f1_m: 0.8776 - val_loss: 0.2969 - val_f1_m: 0.8483\nEpoch 4/6\n351/351 [==============================] - 28s 79ms/step - loss: 0.2184 - f1_m: 0.9068 - val_loss: 0.3149 - val_f1_m: 0.8477\nEpoch 5/6\n351/351 [==============================] - 28s 79ms/step - loss: 0.1904 - f1_m: 0.9166 - val_loss: 0.3330 - val_f1_m: 0.8510\nEpoch 6/6\n351/351 [==============================] - 28s 79ms/step - loss: 0.1516 - f1_m: 0.9376 - val_loss: 0.3687 - val_f1_m: 0.8530\n","output_type":"stream"}]},{"cell_type":"code","source":"n_steps = x_valid.shape[0] // BATCH_SIZE\ntrain_history_2 = model.fit(\n    valid_dataset.repeat(),\n    steps_per_epoch=n_steps,\n    epochs=EPOCHS*2\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T23:28:41.597057Z","iopub.execute_input":"2023-01-10T23:28:41.597443Z","iopub.status.idle":"2023-01-10T23:29:26.033028Z","shell.execute_reply.started":"2023-01-10T23:28:41.597399Z","shell.execute_reply":"2023-01-10T23:29:26.032213Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Epoch 1/6\n39/39 [==============================] - 3s 75ms/step - loss: 0.3068 - f1_m: 0.8680\nEpoch 2/6\n39/39 [==============================] - 29s 75ms/step - loss: 0.2343 - f1_m: 0.8792\nEpoch 3/6\n39/39 [==============================] - 3s 74ms/step - loss: 0.1894 - f1_m: 0.9044\nEpoch 4/6\n39/39 [==============================] - 3s 74ms/step - loss: 0.1537 - f1_m: 0.9154\nEpoch 5/6\n39/39 [==============================] - 3s 74ms/step - loss: 0.1147 - f1_m: 0.9345\nEpoch 6/6\n39/39 [==============================] - 3s 73ms/step - loss: 0.0871 - f1_m: 0.9495\n","output_type":"stream"}]}]}