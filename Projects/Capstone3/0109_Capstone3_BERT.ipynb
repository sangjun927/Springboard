{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/whsos927/notebook96856d2406?scriptVersionId=115969654\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-10T07:01:50.386649Z","iopub.execute_input":"2023-01-10T07:01:50.387096Z","iopub.status.idle":"2023-01-10T07:01:50.423431Z","shell.execute_reply.started":"2023-01-10T07:01:50.386993Z","shell.execute_reply":"2023-01-10T07:01:50.422224Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# load excel\ndata = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:01:50.424992Z","iopub.execute_input":"2023-01-10T07:01:50.425544Z","iopub.status.idle":"2023-01-10T07:01:52.250893Z","shell.execute_reply.started":"2023-01-10T07:01:50.425506Z","shell.execute_reply":"2023-01-10T07:01:52.249847Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install keras","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:01:52.252405Z","iopub.execute_input":"2023-01-10T07:01:52.252755Z","iopub.status.idle":"2023-01-10T07:02:03.558034Z","shell.execute_reply.started":"2023-01-10T07:01:52.252706Z","shell.execute_reply":"2023-01-10T07:02:03.55691Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting keras\n  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n\u001b[K     |████████████████████████████████| 1.7 MB 2.1 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: keras\nSuccessfully installed keras-2.11.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loading Dependencies\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport transformers\n\nfrom tokenizers import BertWordPieceTokenizer\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:02:03.562353Z","iopub.execute_input":"2023-01-10T07:02:03.562888Z","iopub.status.idle":"2023-01-10T07:02:13.132568Z","shell.execute_reply.started":"2023-01-10T07:02:03.562844Z","shell.execute_reply":"2023-01-10T07:02:13.13121Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2023-01-10 07:02:04.248638: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2023-01-10 07:02:04.248796: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:02:13.134945Z","iopub.execute_input":"2023-01-10T07:02:13.135342Z","iopub.status.idle":"2023-01-10T07:02:18.719498Z","shell.execute_reply.started":"2023-01-10T07:02:13.135277Z","shell.execute_reply":"2023-01-10T07:02:18.718542Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Running on TPU  grpc://10.0.0.2:8470\n","output_type":"stream"},{"name":"stderr","text":"2023-01-10 07:02:13.144353: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2023-01-10 07:02:13.147386: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2023-01-10 07:02:13.147430: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n2023-01-10 07:02:13.147458: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (0daa2c71af6f): /proc/driver/nvidia/version does not exist\n2023-01-10 07:02:13.150814: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-01-10 07:02:13.152301: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2023-01-10 07:02:13.182910: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2023-01-10 07:02:13.182986: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30020}\n2023-01-10 07:02:13.202921: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2023-01-10 07:02:13.202987: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30020}\n2023-01-10 07:02:13.204473: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:30020\n","output_type":"stream"},{"name":"stdout","text":"REPLICAS:  8\n","output_type":"stream"}]},{"cell_type":"code","source":"# replace 1 and 0\ndata.sentiment = data.sentiment.replace({'positive': 1, 'negative': 0})","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:02:18.720906Z","iopub.execute_input":"2023-01-10T07:02:18.721157Z","iopub.status.idle":"2023-01-10T07:02:18.784714Z","shell.execute_reply.started":"2023-01-10T07:02:18.721127Z","shell.execute_reply":"2023-01-10T07:02:18.783675Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Creating training and valid dataset by ratio of 1:9\n# valid dataset\nvalid = data.sample(frac = 0.1)\n \n# Creating dataframe with\n# training dataset\ntrain1 = data.drop(valid.index)\n \nprint(\"\\n10% of the given DataFrame:\")\nprint(valid.shape)\n \nprint(\"\\nrest 90% of the given DataFrame:\")\nprint(train1.shape)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:31:44.507888Z","iopub.execute_input":"2023-01-10T07:31:44.508238Z","iopub.status.idle":"2023-01-10T07:31:44.550493Z","shell.execute_reply.started":"2023-01-10T07:31:44.508202Z","shell.execute_reply":"2023-01-10T07:31:44.549434Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"\n10% of the given DataFrame:\n(5000, 2)\n\nrest 90% of the given DataFrame:\n(45000, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"valid.sentiment.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:02:18.805447Z","iopub.execute_input":"2023-01-10T07:02:18.805673Z","iopub.status.idle":"2023-01-10T07:02:18.820192Z","shell.execute_reply.started":"2023-01-10T07:02:18.805646Z","shell.execute_reply":"2023-01-10T07:02:18.819119Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"0    2522\n1    2478\nName: sentiment, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train1.sentiment.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:02:18.821534Z","iopub.execute_input":"2023-01-10T07:02:18.821766Z","iopub.status.idle":"2023-01-10T07:02:18.829815Z","shell.execute_reply.started":"2023-01-10T07:02:18.821739Z","shell.execute_reply":"2023-01-10T07:02:18.829006Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"1    22522\n0    22478\nName: sentiment, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train1 = train1.reset_index(drop=True)\nvalid = valid.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:02:18.832777Z","iopub.execute_input":"2023-01-10T07:02:18.833171Z","iopub.status.idle":"2023-01-10T07:02:18.843129Z","shell.execute_reply.started":"2023-01-10T07:02:18.833139Z","shell.execute_reply":"2023-01-10T07:02:18.842357Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"valid.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:02:18.844489Z","iopub.execute_input":"2023-01-10T07:02:18.844711Z","iopub.status.idle":"2023-01-10T07:02:18.859469Z","shell.execute_reply.started":"2023-01-10T07:02:18.844685Z","shell.execute_reply":"2023-01-10T07:02:18.858479Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                              review  sentiment\n0  I love ghost stories in general, but I PARTICU...          1\n1  Why do movie makers always go against the auth...          0\n2  Even if this film was allegedly a joke in resp...          0\n3  The Black Castle is one of those film's that h...          1\n4  Sandra Bernhard's Without You I'm Nothing, the...          1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I love ghost stories in general, but I PARTICU...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Why do movie makers always go against the auth...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Even if this film was allegedly a joke in resp...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The Black Castle is one of those film's that h...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sandra Bernhard's Without You I'm Nothing, the...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    \"\"\"\n    Encoder for encoding the text into sequence of integers for BERT Input\n    \"\"\"\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:02:18.860943Z","iopub.execute_input":"2023-01-10T07:02:18.861376Z","iopub.status.idle":"2023-01-10T07:02:18.870876Z","shell.execute_reply.started":"2023-01-10T07:02:18.861312Z","shell.execute_reply":"2023-01-10T07:02:18.869812Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#IMP DATA FOR CONFIG\n\nAUTO = tf.data.experimental.AUTOTUNE\n\n\n# Configuration\nEPOCHS = 3\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 192","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:02:18.87264Z","iopub.execute_input":"2023-01-10T07:02:18.873023Z","iopub.status.idle":"2023-01-10T07:02:18.880502Z","shell.execute_reply.started":"2023-01-10T07:02:18.87297Z","shell.execute_reply":"2023-01-10T07:02:18.879656Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# First load the real tokenizer\ntokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\nfast_tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:02:18.883582Z","iopub.execute_input":"2023-01-10T07:02:18.883846Z","iopub.status.idle":"2023-01-10T07:02:21.4367Z","shell.execute_reply.started":"2023-01-10T07:02:18.883817Z","shell.execute_reply":"2023-01-10T07:02:21.43567Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f337cb0dcc064a0e9b9cb1f1777bfabd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bab62719a46841c59bc91faa25ca6f5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25cc80a3ef5645fb918de2dcc3e38286"}},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"Tokenizer(vocabulary_size=119547, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=False, wordpieces_prefix=##)"},"metadata":{}}]},{"cell_type":"code","source":"x_train = fast_encode(train1.review.astype(str), fast_tokenizer, maxlen=MAX_LEN)\nx_valid = fast_encode(valid.review.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n#x_test = fast_encode(test1.review.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n\ny_train = train1.sentiment.values\ny_valid = valid.sentiment.values","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:02:21.438208Z","iopub.execute_input":"2023-01-10T07:02:21.438777Z","iopub.status.idle":"2023-01-10T07:02:44.886805Z","shell.execute_reply.started":"2023-01-10T07:02:21.438727Z","shell.execute_reply":"2023-01-10T07:02:44.885729Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"100%|██████████| 176/176 [00:20<00:00,  8.64it/s]\n100%|██████████| 20/20 [00:02<00:00,  9.54it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .shuffle(3000)\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\n# test_dataset = (\n#     tf.data.Dataset\n#     .from_tensor_slices(x_test)\n#     .batch(BATCH_SIZE)\n# )","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:02:44.88836Z","iopub.execute_input":"2023-01-10T07:02:44.888616Z","iopub.status.idle":"2023-01-10T07:02:45.299498Z","shell.execute_reply.started":"2023-01-10T07:02:44.888587Z","shell.execute_reply":"2023-01-10T07:02:45.298694Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"**Metrics with Accuracy**","metadata":{}},{"cell_type":"code","source":"def build_model(transformer, max_len=512):\n    \"\"\"\n    function for training the BERT model\n    \"\"\"\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:02:45.300596Z","iopub.execute_input":"2023-01-10T07:02:45.30083Z","iopub.status.idle":"2023-01-10T07:02:45.307937Z","shell.execute_reply.started":"2023-01-10T07:02:45.300803Z","shell.execute_reply":"2023-01-10T07:02:45.306995Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"%%time\nwith strategy.scope():\n    transformer_layer = (\n        transformers.TFDistilBertModel\n        .from_pretrained('distilbert-base-multilingual-cased')\n    )\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:04:02.581647Z","iopub.execute_input":"2023-01-10T07:04:02.582544Z","iopub.status.idle":"2023-01-10T07:04:18.978431Z","shell.execute_reply.started":"2023-01-10T07:04:02.582496Z","shell.execute_reply":"2023-01-10T07:04:18.977404Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_layer_norm', 'vocab_projector', 'activation_13']\n- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_word_ids (InputLayer)  [(None, 192)]             0         \n_________________________________________________________________\ntf_distil_bert_model_1 (TFDi TFBaseModelOutput(last_hi 134734080 \n_________________________________________________________________\ntf.__operators__.getitem (Sl (None, 768)               0         \n_________________________________________________________________\ndense (Dense)                (None, 1)                 769       \n=================================================================\nTotal params: 134,734,849\nTrainable params: 134,734,849\nNon-trainable params: 0\n_________________________________________________________________\nCPU times: user 8.35 s, sys: 7.6 s, total: 16 s\nWall time: 16.4 s\n","output_type":"stream"}]},{"cell_type":"code","source":"n_steps = x_train.shape[0] // BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=6\n    #batch_size = BATCH_SIZE\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:04:29.499186Z","iopub.execute_input":"2023-01-10T07:04:29.499568Z","iopub.status.idle":"2023-01-10T07:08:06.766179Z","shell.execute_reply.started":"2023-01-10T07:04:29.499528Z","shell.execute_reply":"2023-01-10T07:08:06.765125Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Epoch 1/6\n351/351 [==============================] - 77s 102ms/step - loss: 0.5315 - accuracy: 0.7062 - val_loss: 0.3219 - val_accuracy: 0.8570\nEpoch 2/6\n351/351 [==============================] - 28s 79ms/step - loss: 0.3176 - accuracy: 0.8600 - val_loss: 0.2902 - val_accuracy: 0.8740\nEpoch 3/6\n351/351 [==============================] - 28s 79ms/step - loss: 0.2652 - accuracy: 0.8855 - val_loss: 0.2818 - val_accuracy: 0.8842\nEpoch 4/6\n351/351 [==============================] - 28s 79ms/step - loss: 0.2243 - accuracy: 0.9070 - val_loss: 0.2906 - val_accuracy: 0.8842\nEpoch 5/6\n351/351 [==============================] - 28s 80ms/step - loss: 0.1827 - accuracy: 0.9269 - val_loss: 0.2974 - val_accuracy: 0.8884\nEpoch 6/6\n351/351 [==============================] - 28s 79ms/step - loss: 0.1488 - accuracy: 0.9415 - val_loss: 0.3235 - val_accuracy: 0.8888\n","output_type":"stream"}]},{"cell_type":"code","source":"n_steps = x_valid.shape[0] // BATCH_SIZE\ntrain_history_2 = model.fit(\n    valid_dataset.repeat(),\n    steps_per_epoch=n_steps,\n    epochs=EPOCHS*2\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:09:58.421127Z","iopub.execute_input":"2023-01-10T07:09:58.422603Z","iopub.status.idle":"2023-01-10T07:10:42.018671Z","shell.execute_reply.started":"2023-01-10T07:09:58.42255Z","shell.execute_reply":"2023-01-10T07:10:42.017731Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Epoch 1/6\n39/39 [==============================] - 3s 74ms/step - loss: 0.3013 - accuracy: 0.8784\nEpoch 2/6\n39/39 [==============================] - 29s 74ms/step - loss: 0.2286 - accuracy: 0.9087\nEpoch 3/6\n39/39 [==============================] - 3s 74ms/step - loss: 0.1788 - accuracy: 0.9298\nEpoch 4/6\n39/39 [==============================] - 3s 74ms/step - loss: 0.1407 - accuracy: 0.9456\nEpoch 5/6\n39/39 [==============================] - 3s 74ms/step - loss: 0.1049 - accuracy: 0.9598\nEpoch 6/6\n39/39 [==============================] - 3s 74ms/step - loss: 0.0851 - accuracy: 0.9690\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Metrics with recall**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import backend as K\n#tensorflow.keras.\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:18:01.469583Z","iopub.execute_input":"2023-01-10T07:18:01.470361Z","iopub.status.idle":"2023-01-10T07:18:01.476216Z","shell.execute_reply.started":"2023-01-10T07:18:01.470268Z","shell.execute_reply":"2023-01-10T07:18:01.475474Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def build_model(transformer, max_len=512):\n    \"\"\"\n    function for training the BERT model\n    \"\"\"\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=[recall_m])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:18:21.869121Z","iopub.execute_input":"2023-01-10T07:18:21.870594Z","iopub.status.idle":"2023-01-10T07:18:21.878242Z","shell.execute_reply.started":"2023-01-10T07:18:21.87053Z","shell.execute_reply":"2023-01-10T07:18:21.877229Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"%%time\nwith strategy.scope():\n    transformer_layer = (\n        transformers.TFDistilBertModel\n        .from_pretrained('distilbert-base-multilingual-cased')\n    )\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:18:24.398537Z","iopub.execute_input":"2023-01-10T07:18:24.399553Z","iopub.status.idle":"2023-01-10T07:18:37.815546Z","shell.execute_reply.started":"2023-01-10T07:18:24.399487Z","shell.execute_reply":"2023-01-10T07:18:37.814588Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_layer_norm', 'vocab_projector', 'activation_13']\n- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_word_ids (InputLayer)  [(None, 192)]             0         \n_________________________________________________________________\ntf_distil_bert_model_3 (TFDi TFBaseModelOutput(last_hi 134734080 \n_________________________________________________________________\ntf.__operators__.getitem_2 ( (None, 768)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 769       \n=================================================================\nTotal params: 134,734,849\nTrainable params: 134,734,849\nNon-trainable params: 0\n_________________________________________________________________\nCPU times: user 6.47 s, sys: 5.74 s, total: 12.2 s\nWall time: 13.4 s\n","output_type":"stream"}]},{"cell_type":"code","source":"n_steps = x_train.shape[0] // BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=6\n    #batch_size = BATCH_SIZE\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:18:43.506871Z","iopub.execute_input":"2023-01-10T07:18:43.50721Z","iopub.status.idle":"2023-01-10T07:22:26.249615Z","shell.execute_reply.started":"2023-01-10T07:18:43.507178Z","shell.execute_reply":"2023-01-10T07:22:26.248447Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Epoch 1/6\n351/351 [==============================] - 81s 103ms/step - loss: 0.5414 - recall_m: 0.6677 - val_loss: 0.3220 - val_recall_m: 0.8007\nEpoch 2/6\n351/351 [==============================] - 28s 80ms/step - loss: 0.3157 - recall_m: 0.8628 - val_loss: 0.2890 - val_recall_m: 0.8551\nEpoch 3/6\n351/351 [==============================] - 28s 80ms/step - loss: 0.2653 - recall_m: 0.8915 - val_loss: 0.3085 - val_recall_m: 0.9223\nEpoch 4/6\n351/351 [==============================] - 29s 82ms/step - loss: 0.2263 - recall_m: 0.9148 - val_loss: 0.2941 - val_recall_m: 0.8818\nEpoch 5/6\n351/351 [==============================] - 28s 80ms/step - loss: 0.1871 - recall_m: 0.9305 - val_loss: 0.3055 - val_recall_m: 0.8965\nEpoch 6/6\n351/351 [==============================] - 28s 79ms/step - loss: 0.1527 - recall_m: 0.9414 - val_loss: 0.3319 - val_recall_m: 0.8909\n","output_type":"stream"}]},{"cell_type":"code","source":"n_steps = x_valid.shape[0] // BATCH_SIZE\ntrain_history_2 = model.fit(\n    valid_dataset.repeat(),\n    steps_per_epoch=n_steps,\n    epochs=EPOCHS*2\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:22:36.030475Z","iopub.execute_input":"2023-01-10T07:22:36.030823Z","iopub.status.idle":"2023-01-10T07:23:20.14918Z","shell.execute_reply.started":"2023-01-10T07:22:36.030783Z","shell.execute_reply":"2023-01-10T07:23:20.148231Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Epoch 1/6\n39/39 [==============================] - 3s 75ms/step - loss: 0.3093 - recall_m: 0.8816\nEpoch 2/6\n39/39 [==============================] - 29s 75ms/step - loss: 0.2369 - recall_m: 0.8893\nEpoch 3/6\n39/39 [==============================] - 3s 75ms/step - loss: 0.1904 - recall_m: 0.9131\nEpoch 4/6\n39/39 [==============================] - 3s 74ms/step - loss: 0.1609 - recall_m: 0.9242\nEpoch 5/6\n39/39 [==============================] - 3s 74ms/step - loss: 0.1386 - recall_m: 0.9363\nEpoch 6/6\n39/39 [==============================] - 3s 74ms/step - loss: 0.1068 - recall_m: 0.9454\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Metrics with F1 Score**","metadata":{}},{"cell_type":"code","source":"def f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:26:05.715386Z","iopub.execute_input":"2023-01-10T07:26:05.71594Z","iopub.status.idle":"2023-01-10T07:26:05.72318Z","shell.execute_reply.started":"2023-01-10T07:26:05.715897Z","shell.execute_reply":"2023-01-10T07:26:05.72199Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def build_model(transformer, max_len=512):\n    \"\"\"\n    function for training the BERT model\n    \"\"\"\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=[f1_m])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:26:08.441515Z","iopub.execute_input":"2023-01-10T07:26:08.44192Z","iopub.status.idle":"2023-01-10T07:26:08.450413Z","shell.execute_reply.started":"2023-01-10T07:26:08.441877Z","shell.execute_reply":"2023-01-10T07:26:08.449208Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"%%time\nwith strategy.scope():\n    transformer_layer = (\n        transformers.TFDistilBertModel\n        .from_pretrained('distilbert-base-multilingual-cased')\n    )\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:26:12.445401Z","iopub.execute_input":"2023-01-10T07:26:12.445801Z","iopub.status.idle":"2023-01-10T07:26:23.49455Z","shell.execute_reply.started":"2023-01-10T07:26:12.44576Z","shell.execute_reply":"2023-01-10T07:26:23.493495Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_layer_norm', 'vocab_projector', 'activation_13']\n- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_word_ids (InputLayer)  [(None, 192)]             0         \n_________________________________________________________________\ntf_distil_bert_model_5 (TFDi TFBaseModelOutput(last_hi 134734080 \n_________________________________________________________________\ntf.__operators__.getitem_4 ( (None, 768)               0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 1)                 769       \n=================================================================\nTotal params: 134,734,849\nTrainable params: 134,734,849\nNon-trainable params: 0\n_________________________________________________________________\nCPU times: user 6.26 s, sys: 4.69 s, total: 11 s\nWall time: 11 s\n","output_type":"stream"}]},{"cell_type":"code","source":"n_steps = x_train.shape[0] // BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=6\n    #batch_size = BATCH_SIZE\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:26:28.263957Z","iopub.execute_input":"2023-01-10T07:26:28.264352Z","iopub.status.idle":"2023-01-10T07:30:12.724501Z","shell.execute_reply.started":"2023-01-10T07:26:28.264287Z","shell.execute_reply":"2023-01-10T07:30:12.723436Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Epoch 1/6\n351/351 [==============================] - 82s 103ms/step - loss: 0.5646 - f1_m: 0.6430 - val_loss: 0.3300 - val_f1_m: 0.8406\nEpoch 2/6\n351/351 [==============================] - 28s 80ms/step - loss: 0.3205 - f1_m: 0.8503 - val_loss: 0.3043 - val_f1_m: 0.8383\nEpoch 3/6\n351/351 [==============================] - 29s 82ms/step - loss: 0.2694 - f1_m: 0.8761 - val_loss: 0.2808 - val_f1_m: 0.8607\nEpoch 4/6\n351/351 [==============================] - 28s 81ms/step - loss: 0.2269 - f1_m: 0.9016 - val_loss: 0.2876 - val_f1_m: 0.8708\nEpoch 5/6\n351/351 [==============================] - 28s 80ms/step - loss: 0.1869 - f1_m: 0.9184 - val_loss: 0.3099 - val_f1_m: 0.8673\nEpoch 6/6\n351/351 [==============================] - 28s 80ms/step - loss: 0.1521 - f1_m: 0.9353 - val_loss: 0.3425 - val_f1_m: 0.8681\n","output_type":"stream"}]},{"cell_type":"code","source":"n_steps = x_valid.shape[0] // BATCH_SIZE\ntrain_history_2 = model.fit(\n    valid_dataset.repeat(),\n    steps_per_epoch=n_steps,\n    epochs=EPOCHS*2\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-10T07:30:40.265761Z","iopub.execute_input":"2023-01-10T07:30:40.266137Z","iopub.status.idle":"2023-01-10T07:31:22.832905Z","shell.execute_reply.started":"2023-01-10T07:30:40.266098Z","shell.execute_reply":"2023-01-10T07:31:22.832055Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Epoch 1/6\n39/39 [==============================] - 3s 75ms/step - loss: 0.3021 - f1_m: 0.8732\nEpoch 2/6\n39/39 [==============================] - 27s 75ms/step - loss: 0.2220 - f1_m: 0.8938\nEpoch 3/6\n39/39 [==============================] - 3s 75ms/step - loss: 0.1814 - f1_m: 0.9131\nEpoch 4/6\n39/39 [==============================] - 3s 74ms/step - loss: 0.1353 - f1_m: 0.9337\nEpoch 5/6\n39/39 [==============================] - 3s 74ms/step - loss: 0.1060 - f1_m: 0.9433\nEpoch 6/6\n39/39 [==============================] - 3s 74ms/step - loss: 0.0766 - f1_m: 0.9541\n","output_type":"stream"}]}]}